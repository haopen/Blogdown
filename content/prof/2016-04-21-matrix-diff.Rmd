---
title: 矩阵求导 z
date: 2016-04-21
categories: ["6-经济 - 数学与动态"]
tags: ["6-矩阵", "6-导数"]
slug: matrix-diff
---

矩阵求导好像读书的时候都没学过，因为讲矩阵的课程上不讲求导，讲求导的课又不提矩阵。如果从事机器学习方面的工作，那就一定会遇到矩阵求导的东西。[维基百科](http://en.wikipedia.org/wiki/Matrix_calculus)上， 根据$y,\pmb{y},Y$与$x,\pmb{x},X$的不同类型（实值，向量，矩阵），给出了具体的求导公式，以及一堆相关的公式，查起来都费劲。

<!-- more -->

*矩阵求导*

| 类型 | 单值 | 向量 | 矩阵 |
|:------|:-----|---------|:------|
|  单值  |  $\partial y/\partial x$        |  $\partial y/\partial x$               |  $\partial \pmb{y}/\partial Y$  |
|  向量  |  $\partial y/\partial \pmb{x}$  |  $\partial \pmb{y}/\partial \pmb{x}$   |     |
|  矩阵  |  $\partial y/\partial X$        |        |       |

其实在实际的机器学习工作中，最常用到的就是实值函数$y$对向量$\pmb{x}$的求导，定义如下（其实就是$y$对向量$\pmb{x}$的每一个元素求导）：

$$
\frac{\partial y}{\partial \pmb{x}}=
\begin{bmatrix}
    \dfrac{\partial y}{\partial  x_1}\\
    \dfrac{\partial y}{\partial  x_2}\\
    \vdots\\
    \dfrac{\partial y}{\partial  x_n}
\end{bmatrix}
$$

实值函数$y$对矩阵$X$求导也类似：

$$
\frac{\partial y}{\partial X}=
\begin{bmatrix}
          \dfrac{\partial y}{\partial   x_{11}} & \dfrac{\partial y}{\partial   x_{12}}& \cdots &\dfrac{\partial y}{\partial   x_{1n}}\\  
          \dfrac{\partial y}{\partial   x_{21}} & \dfrac{\partial y}{\partial   x_{22}}&\cdots &\dfrac{\partial y}{\partial   x_{2n}}\\  
          \vdots & \vdots & \ddots & \vdots\\  
          \dfrac{\partial y}{\partial   x_{n1}} & \dfrac{\partial y}{\partial   x_{n2}}&\cdots &\dfrac{\partial y}{\partial   x_{nn}}\\  
\end{bmatrix}
$$

因为有监督的机器学习的一般套路是给定输入$\pmb{x}$，选择一个模型$f$作为决策函数，由$f(\pmb{x})$预测出$\hat{y}$。而要得到$f$的参数$\theta$，需要定义一个 loss 函数来定义当前的预测值$\hat{y}$和实际值$y$之间的接近程度，模型学习的过程就是求使得 loss 函数$L(f(\pmb{x}),y)$最小的参数$\theta$。这是一个最优化的问题，实际应用中都是用和梯度相关的最优化方法，如梯度下降，共轭梯度，拟牛顿法等等。为方便推倒有以下公式：

$$
\frac{\partial \beta^T\pmb{x}}{\partial \pmb{x}}  =\beta, \quad\frac{\partial \pmb{x}^T\pmb{x}}{\partial \pmb{x}}  =2\pmb{x}, \quad\frac{\partial \pmb{x}^T A\pmb{x}}{\partial \pmb{x}}  =(\mathbf{A}+\mathbf{A}^T)\pmb{x}
$$

其实只要掌握上面的公式，就能搞定很多问题了。

为了方便推导，下面列出一些机器学习中常用的求导公式，其中 andrew ng 那一套用矩阵迹的方法还是挺不错的，矩阵的迹也是实值的，而一个实数的迹等于其本身，实际工作中可以将 loss 函数转化成迹，然后再求导，可能会简化推导的步骤。 

$$\text{tr}(a)=a$$
$$\text{tr}(AB)=\text{tr}(BA)$$
$$\text{tr}(ABC)=\text{tr}(CAB)=\text{tr}(BCA)$$
$$\frac{\partial{\text{tr}(AB)}}{A}=B^T$$
$$\text{tr}(A)=\text{tr}(A^T)$$
$$\frac{\partial{\text{tr}(ABA^TC)}}{A}=CAB+C^TAB^T$$

以上只是一些最基本的公式，能够解决一些问题，主要是减少大家对矩阵求导的恐惧感。关于矩阵方面的更多信息可以参考上面的 [wiki](http://en.wikipedia.org/wiki/Matrix_calculus) 链接以及《Matrix cookbook》。

# 参考资料

> **未完成工作**：1, 3, 4 中的内容还有待进一步整理，事实上整理完成 1 中的内容就完全足够，但第 4 项中关于 Notation 的总结也不错。

1. <https://en.wikipedia.org/wiki/Matrix_calculus>
2. <http://blog.sina.com.cn/s/blog_8eac0b290101fsqb.html>
3. <http://www.voidcn.com/blog/liuuze5/article/p-5037874.html>
4. <http://blog.csdn.net/u012045426/article/details/52343676>
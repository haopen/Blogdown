---
title: 岭回归和最小二乘法的区别是什么？z
author: 慧航
date: 2015-07-05
categories: ["6-经济 - 计量"]
tags: ["6-岭回归", "6-OLS", "6-最小二乘"]
slug: ridge-regression
---

**原文地址**：<https://www.zhihu.com/question/28221429>

> **说明**：$X'X$在相关程度高的时候，$|X'X|$非常接近于 0，这导致$|X'X|^{-1}$很大，从而不一样的样本数量对应的结果之间差别会非常大，模型的拟合结果非常不稳定，参数估计量的方差也增大，对参数的估计会不准确。

<!-- more -->

# 子元

关于稳定性再补充一下。当回归变量$X$不是列满秩的时候，我们固然需要通过正则化来获得唯一解：

$$\min_{\pmb{\beta}} \|y-X\pmb{\beta}\|^2\Rightarrow\min_{\pmb{\beta}} \|y-X\pmb{\beta}\|^2+\lambda\|\pmb{\beta}\|^2$$

但即使$X$列满秩，我们来看看当有其中两列相关程度很高时，会发生什么。

比方说一个自变量是身高$x_1$，一个自变量是体重$x_2$，假设因变量y是某种性激素的水平（或者别的什么跟身体发育相关的东西，随便举的例子）。虽然我们拟合后能得到唯一解$\hat{y}=ax_1+bx_2+c$，但由于$x_1$和$x_2$高度相关，所以$a$和$b$之间存在`互相抵消`的效应：你可以把$a$弄成一个很大的正数，同时把$b$弄成一个绝对值很大的负数，最终$\hat{y}$可能不会改变多少。这会导致用不同人群拟合出来的$a$和$b$差别可能会很大，模型的可解释性就大大降低了。怎么办？最简单就是给一个限制，令$a^2+b^2\leqslant t$，这正好就是岭回归。

[The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) 第 63 页有提到这一点：

> When there are many correlated variables in a linear regression model, their coefficients can become poorly determined and exhibit high variance. A wildly large positive coefficient on one variable can be canceled by a similarly large negative coefficient on its correlated cousin. By imposing a size constraint on the coefficients, as in (3.42), this problem is alleviated.


我觉得这个才是岭回归的 motivation，`正态先验`充其量只是一个概率解释。

<blockquote class="gray">
**司马木**：确实如此，相比之下，LASSO 就差些了。

<blockquote class="white">
**子元**：没有什么差不差。LASSO 进一步追求稀疏性，不像岭回归那样按某个固定因子来收缩。

</blockquote>
</blockquote>

<blockquote class="gray">
**fresh**：谢谢楼主的举例。加上了正则化 L2 范数，其实就是认为参数服从先验分布（高斯分布），然后就把系数给限制住了，这样就不会出现某个系数可大可小即不稳定。对吧？但是我对二乘法不了解，尤其是不是列满秩与否所带来的问题更不了解？请指教如何有针对的解决啊？

</blockquote>

# 亲爱的龙哥

最小二乘法是对普通线性回归参数估计的一种方法，目标是是 loss function 达到最小，而此时的 loss function 是误差平方和。
岭回归和普通线性回归的区别，我们可以从三种方式来看。

1. 最优化问题的不同

$$\hat{\pmb{\beta}}_{\text{ridge}}=\mathop{\arg\,\min}_{\pmb{\beta}}\left\{\sum^N_{i=1}\left(y_i-\beta_0-\sum^p_{j=1}x_{ij}\beta_j\right)^2+\lambda\sum^p_{j=1}\beta^2_j\right\}$$

对于岭回归，我们的最优化问题多了后面这些$\beta$的平方和。

多元线性回归的 OLS 回归不仅仅可以看成是对 loss function 的最小化，得出的结果也是$Y$在$X$的线性空间上的投影。

2. 从多变量回归的变量选择来说，普通的多元线性回归要做的是变量的剔除和筛选，而岭回归是一种 shrinkage 的方法，就是收缩。这是什么意思呢， 比如做普通线性回归时候，如果某个变量 t 检验不显著，我们通常会将它剔除再做回归，如此往复（**stepwise**)，最终筛选留下得到一个我们满意回归方程，但是在做岭回归的时候，我们并没有做变量的剔除，而是将这个变量的系数$\beta$向$0$“收缩”，使得这个变量在回归方程中的影响变的很小。与普通的多元线性回归相比，岭回归的变化更加 smooth，或者说 continuous。从这点上来说活，岭回归只是 shrinkage methods 中的一种，大家常说的 LASSO 回归（貌似叫套索回归）其实也属于这种方法。

3. 从计算的角度，有人提到了多元线性回归的 OLS 估计是

$$\hat{\pmb{\beta}}=(X'X)X'\pmb{y}$$

当存在很强的多重共线性时$X'X$是不可逆（或者接近不可逆）的，但是岭回归系数估计是


$$\hat{\pmb{\beta}}_{\text{ridge}}=(X'X+\lambda I)^{-1}X'\pmb{y}$$

此时虽然对系数的估计是有偏的，但是提高了稳定性。

# bh lin

先从优化的角度讲讲这个问题。

最小二乘回归求解的最小化问题是：$\min||y-Ax||^2$，这个问题解存在且唯一的条件是$A$列满秩：

$$\mathrm{rank}(A)=\mathrm{dim}(x)$$

当此条件不满足时，你需要添加一些额外的假设来达到唯一的解。比如岭回归在 cost function 中加$L^2$的测度项。

而$A$不满足列满秩这个条件在回归上的可以简单理解为你所有的样本没有办法提供给你足够的有效的信息^[这句感觉不是太恰当，因为也可以理解为信息不足，但显示不是信息不足，而是信息的冗余问题较严重。]。这时候，你就需要一些额外的假设。从 Bayesian 的角度，比如你假设$x$应该是服从多元正态分布$\mathrm{N}(0,\Sigma_x)$,那么根据 Bayes theorem，你可以推知岭回归的结果就是MAP（maximum a priori）的估计。

# 少吃多有味

简单说，岭回归是带二范数惩罚的最小二乘回归。ols 方法中，

$$b=(X'X)^{-1}X'Y$$

$X'X$不能为 0。当变量之间的相关性较强时，$X'X$很小，甚至趋于 0。

岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对`病态数据`的拟合要强于 OLS。本质是在自变量信息矩阵的主对角线元素上人为地加入一个非负因子。即：

$$b(\lambda)=(X'X+\lambda I)^{-1}X'Y$$

当$\lambda=0$时，$b(\lambda)=b$。$b(\lambda)$中各元素$b_i(\lambda)$的绝对值均趋于不断变小（由于自变量间的相关，个别$b_i(\lambda)$可能有小范围的向上波动或改变正、负号)，$\lambda$增大时，它们对$b_i$的偏差也将愈来愈大；如果$\lambda\to\infty$，则$b(\lambda)\to 0$。$b(\lambda)$随$\lambda$的改变而变化的轨迹，就称为**岭迹**。

应用场景就是处理高度相关的数据。画出岭迹图，选取稳定的那一段的 lambda 就好了。

1. [Kernel ridge Regression](http://www.ics.uci.edu/~welling/teaching/KernelsICS273B/Kernel-Ridge.pdf)

<blockquote class="gray">
**白羽**：计量经济学的文章很少见到岭回归。我觉得，这里就出现了统计学和计量经济学的一个很大的区别了。岭回归我理解它的本质在于牺牲无偏性和一致性来换取有效性，最近听的一个讲座中统计研究者也会通过它来估计高维模型。但是在计量经济学里面，一致性始终是第一位的，不一致的估计方法造成的偏差在实证中造成的偏差很可能会带来严重后果。因此，计量经济学对共线性问题更多的则是“无为而治”。

</blockquote>

# 路路

昨天做模型刚看到这个问题，来自推酷的一篇文章写得挺明白：

当$X$不是列满秩，或者某些列之间的线性相关性比较大时，$X'X$的行列式接近于$0$，即$X'X$接近于非奇异，计算$(X'X)^{-1}$ 时误差会很大。此时传统的最小二乘法缺乏稳定性与可靠性。

岭回归是对最小二乘回归的一种补充，它损失了~~`无偏性`~~，来换取高的`数值稳定性`，从而得到较高的计算精度。当$X'X$的行列式接近于$0$时，我们将其主对角元素都加上一个数$k$，可以使矩阵为非奇异的风险大降低。于是：随着$k$的增大，$B(k)$中各元素$b_i(k)$的绝对值均趋于不断变小，它们相对于正确值$b_i$的偏差也越来越大。$k$趋于无穷大时，$B(k)$趋于$0$。$b(k)$随$k$的改变而变化的轨迹，就称为**岭迹**。实际计算中可选非常多的$k$值，做出一个**岭迹图**，看看这个图在取哪个值的时候变稳定了，那就确定k值了。X不满足列满秩，换句话就是说样本向量之间具有高度的相关性（如果每一列是一个向量的话）。

> **技巧**：遇到列向量相关的情形，岭回归是一种处理方法，也可以用主成分分析 PCA 来进行降维。

# guosc

岭回归就是在最小二乘法的后面加上正则项，正则项是对待求系数的惩罚以避免过拟合的发生。可以从好几个方面来解释正则项的作用。

1. 当训练数据较少时，$X^TX$不是满秩的，所以可能有很多可行解，没法找到最优的解。此时加上正则项能使矩阵满秩，也就有最优解了；
2. 直观上来说训练数据较少时容易发生过拟合，过拟合曲线会尽可能拟合所有数据点，包括噪音点，此时由于函数的导数较大，因此系数较大，为了避免过拟合需要减小系数，正则化就是通过对系数进行惩罚以减小系数。这样得到的会是一条光滑的曲线，会有较好的泛化性能；
3. 具体到岭回归，其正则项是二范数，从贝叶斯的角度看，相当于对系数 omega 添加了一个先验信息，所以会有更好的表现。所以，我认为训练数据较少时（具体多少我也不知道）岭回归会更好一点。